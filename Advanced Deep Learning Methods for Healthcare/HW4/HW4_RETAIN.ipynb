{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "e571336ae0a912e1d78e66d76be88730",
          "grade": false,
          "grade_id": "cell-52506fc51faeb1a1",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "# HW4 RETAIN\n",
        "\n",
        "## Overview\n",
        "\n",
        "Previously, you tried heart failure predictioin with classical machine learning models, neural network (NN), and recurrent neural network (RNN). \n",
        "\n",
        "In this question, you will try a different approach. You will implement RETAIN, a RNN model with attention mechanism, proposed by Choi et al. in the paper [RETAIN: An Interpretable Predictive Model for Healthcare using Reverse Time Attention Mechanism](https://arxiv.org/abs/1608.05745)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "32e72084469253ba7b428e2d0bd46613",
          "grade": false,
          "grade_id": "cell-dcd6c662fba70926",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-12-14T05:55:27.727559Z",
          "start_time": "2021-12-14T05:55:27.089731Z"
        },
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "478f107350e8fe54bd6fc439ffe45f4a",
          "grade": false,
          "grade_id": "cell-4fe346254a16fed8",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pickle\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-12-14T05:55:27.733337Z",
          "start_time": "2021-12-14T05:55:27.729437Z"
        },
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "5e279bf7debce23a92dd79a85312900e",
          "grade": false,
          "grade_id": "cell-6004290bcf81c83a",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "# set seed\n",
        "seed = 24\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "\n",
        "# define data path\n",
        "DATA_PATH = \"../HW4_RETAIN-lib/data/\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "212e434fd38be5ca223e82a1e1fddf5b",
          "grade": false,
          "grade_id": "cell-71f2f1fcbf0214c3",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "55404ea2aa4102d191610a8bd509d630",
          "grade": false,
          "grade_id": "cell-f24c5a8a552afa64",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "## About Raw Data\n",
        "\n",
        "We will perform heart failure prediction using the diagnosis codes. We will use the same dataset from HW3 RNN, which is synthesized from [MIMIC-III](https://mimic.physionet.org/gettingstarted/access/).\n",
        "\n",
        "The data has been preprocessed for you. Let us load them and take a look."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-12-14T05:55:27.742509Z",
          "start_time": "2021-12-14T05:55:27.735038Z"
        },
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "d0dd1f4063e22be64f2709deffde7a7b",
          "grade": false,
          "grade_id": "cell-0d031c45ba4a787e",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "pids = pickle.load(open(os.path.join(DATA_PATH,'train/pids.pkl'), 'rb'))\n",
        "vids = pickle.load(open(os.path.join(DATA_PATH,'train/vids.pkl'), 'rb'))\n",
        "hfs = pickle.load(open(os.path.join(DATA_PATH,'train/hfs.pkl'), 'rb'))\n",
        "seqs = pickle.load(open(os.path.join(DATA_PATH,'train/seqs.pkl'), 'rb'))\n",
        "types = pickle.load(open(os.path.join(DATA_PATH,'train/types.pkl'), 'rb'))\n",
        "rtypes = pickle.load(open(os.path.join(DATA_PATH,'train/rtypes.pkl'), 'rb'))\n",
        "\n",
        "assert len(pids) == len(vids) == len(hfs) == len(seqs) == 1000\n",
        "assert len(types) == 619"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "1427cf82d51752cd4e90e7d483141ffe",
          "grade": false,
          "grade_id": "cell-66a0abe057d9ca85",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "where\n",
        "\n",
        "- `pids`: contains the patient ids\n",
        "- `vids`: contains a list of visit ids for each patient\n",
        "- `hfs`: contains the heart failure label (0: normal, 1: heart failure) for each patient\n",
        "- `seqs`: contains a list of visit (in ICD9 codes) for each patient\n",
        "- `types`: contains the map from ICD9 codes to ICD-9 labels\n",
        "- `rtypes`: contains the map from ICD9 labels to ICD9 codes\n",
        "\n",
        "Let us take a patient as an example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-12-14T05:55:27.750432Z",
          "start_time": "2021-12-14T05:55:27.745342Z"
        },
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "a0d3847b04bd6dadf5ff567bee973690",
          "grade": false,
          "grade_id": "cell-ae331190a6d48106",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "# take the 3rd patient as an example\n",
        "\n",
        "print(\"Patient ID:\", pids[3])\n",
        "print(\"Heart Failure:\", hfs[3])\n",
        "print(\"# of visits:\", len(vids[3]))\n",
        "for visit in range(len(vids[3])):\n",
        "    print(f\"\\t{visit}-th visit id:\", vids[3][visit])\n",
        "    print(f\"\\t{visit}-th visit diagnosis labels:\", seqs[3][visit])\n",
        "    print(f\"\\t{visit}-th visit diagnosis codes:\", [rtypes[label] for label in seqs[3][visit]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "f6484d87ff7b7dcc915b95cd6496a47f",
          "grade": false,
          "grade_id": "cell-945119717fb61cc7",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "Note that `seqs` is a list of list of list. That is, `seqs[i][j][k]` gives you the k-th diagnosis codes for the j-th visit for the i-th patient.\n",
        "\n",
        "And you can look up the meaning of the ICD9 code online. For example, `DIAG_276` represetns *disorders of fluid electrolyte and acid-base balance*.\n",
        "\n",
        "Further, let see number of heart failure patients."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-12-14T05:55:27.754867Z",
          "start_time": "2021-12-14T05:55:27.751763Z"
        },
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "06d423d6893adfdc928488e362a86f3a",
          "grade": false,
          "grade_id": "cell-e6d339169f140694",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "print(\"number of heart failure patients:\", sum(hfs))\n",
        "print(\"ratio of heart failure patients: %.2f\" % (sum(hfs) / len(hfs)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "1708c6868ac5c3aeec8fef0d390ab830",
          "grade": false,
          "grade_id": "cell-308c526175fdb62e",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "## 1 Build the dataset [15 points]\n",
        "\n",
        "### 1.1 CustomDataset [5 points]\n",
        "\n",
        "This is the same as HW3 RNN.\n",
        "\n",
        "First, let us implement a custom dataset using PyTorch class `Dataset`, which will characterize the key features of the dataset we want to generate.\n",
        "\n",
        "We will use the sequences of diagnosis codes `seqs` as input and heart failure `hfs` as output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-12-14T05:55:27.762221Z",
          "start_time": "2021-12-14T05:55:27.757868Z"
        },
        "deletable": false
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset\n",
        "\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "    \n",
        "    def __init__(self, seqs, hfs):\n",
        "        self.x = seqs\n",
        "        self.y = hfs\n",
        "    \n",
        "    def __len__(self):\n",
        "        \n",
        "        \"\"\"\n",
        "        TODO: Return the number of samples (i.e. patients).\n",
        "        \"\"\"\n",
        "        \n",
        "        # your code here\n",
        "        raise NotImplementedError\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        \n",
        "        \"\"\"\n",
        "        TODO: Generates one sample of data.\n",
        "        \n",
        "        Note that you DO NOT need to covert them to tensor as we will do this later.\n",
        "        \"\"\"\n",
        "        \n",
        "        # your code here\n",
        "        raise NotImplementedError\n",
        "        \n",
        "\n",
        "dataset = CustomDataset(seqs, hfs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-12-14T05:55:27.772667Z",
          "start_time": "2021-12-14T05:55:27.763961Z"
        },
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "9698147ad965e9a4336317e632e30259",
          "grade": true,
          "grade_id": "cell-cc0baa6c9dadef8c",
          "locked": true,
          "points": 5,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "'''\n",
        "AUTOGRADER CELL. DO NOT MODIFY THIS.\n",
        "'''\n",
        "\n",
        "dataset = CustomDataset(seqs, hfs)\n",
        "\n",
        "assert len(dataset) == 1000\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "57bddbf82df12dd184ecbf766b0c0b9f",
          "grade": false,
          "grade_id": "cell-de0d816943d88377",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "### 1.2 Collate Function [5 points]\n",
        "\n",
        "This is the same as HW3 RNN.\n",
        "\n",
        "As you note that, we do not convert the data to tensor in the built `CustomDataset`. Instead, we will do this using a collate function `collate_fn()`. \n",
        "\n",
        "This collate function `collate_fn()` will be called by `DataLoader` after fetching a list of samples using the indices from `CustomDataset` to collate the list of samples into batches.\n",
        "\n",
        "For example, assume the `DataLoader` gets a list of two samples.\n",
        "\n",
        "```\n",
        "[ [ [0, 1, 2], [8, 0] ], \n",
        "  [ [12, 13, 6, 7], [12], [23, 11] ] ]\n",
        "```\n",
        "\n",
        "where the first sample has two visits `[0, 1, 2]` and `[8, 0]` and the second sample has three visits `[12, 13, 6, 7]`, `[12]`, and `[23, 11]`.\n",
        "\n",
        "The collate function `collate_fn()` is supposed to pad them into the same shape (3, 4), where 3 is the maximum number of visits and 4 is the maximum number of diagnosis codes.\n",
        "\n",
        "``` \n",
        "[ [ [0, 1, 2, *0*], [8, 0, *0*, *0*], [*0*, *0*, *0*, *0*]  ], \n",
        "  [ [12, 13, 6, 7], [12, *0*, *0*, *0*], [23, 11, *0*, *0*] ] ]\n",
        "```\n",
        "\n",
        "Further, the padding information will be stored in a mask with the same shape, where 1 indicates that the diagnosis code at this position is from the original input, and 0 indicates that the diagnosis code at this position is the padded value.\n",
        "\n",
        "```\n",
        "[ [ [1, 1, 1, 0], [1, 1, 0, 0], [0, 0, 0, 0] ], \n",
        "  [ [1, 1, 1, 1], [1, 0, 0, 0], [1, 1, 0, 0] ] ]\n",
        "```\n",
        "\n",
        "Lastly, we will have another diagnosis sequence in reversed time. This will be used in our RNN model for masking. Note that we only flip the true visits.\n",
        "\n",
        "``` \n",
        "[ [ [8, 0, *0*, *0*], [0, 1, 2, *0*], [*0*, *0*, *0*, *0*]  ], \n",
        "  [ [23, 11, *0*, *0*], [12, *0*, *0*, *0*], [12, 13, 6, 7] ] ]\n",
        "```\n",
        "\n",
        "And a reversed mask as well.\n",
        "\n",
        "```\n",
        "[ [ [1, 1, 0, 0], [1, 1, 1, 0], [0, 0, 0, 0] ], \n",
        "  [ [1, 1, 0, 0], [1, 0, 0, 0], [1, 1, 1, 1], ] ]\n",
        "```\n",
        "\n",
        "We need to pad the sequences into the same length so that we can do batch training on GPU. And we also need this mask so that when training, we can ignored the padded value as they actually do not contain any information."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-12-14T05:55:27.782724Z",
          "start_time": "2021-12-14T05:55:27.774569Z"
        },
        "deletable": false
      },
      "outputs": [],
      "source": [
        "def collate_fn(data):\n",
        "    \"\"\"\n",
        "    TODO: Collate the the list of samples into batches. For each patient, you need to pad the diagnosis\n",
        "        sequences to the sample shape (max # visits, max # diagnosis codes). The padding infomation\n",
        "        is stored in `mask`.\n",
        "    \n",
        "    Arguments:\n",
        "        data: a list of samples fetched from `CustomDataset`\n",
        "        \n",
        "    Outputs:\n",
        "        x: a tensor of shape (# patiens, max # visits, max # diagnosis codes) of type torch.long\n",
        "        masks: a tensor of shape (# patiens, max # visits, max # diagnosis codes) of type torch.bool\n",
        "        rev_x: same as x but in reversed time. This will be used in our RNN model for masking \n",
        "        rev_masks: same as mask but in reversed time. This will be used in our RNN model for masking\n",
        "        y: a tensor of shape (# patiens) of type torch.float\n",
        "        \n",
        "    Note that you can obtains the list of diagnosis codes and the list of hf labels\n",
        "        using: `sequences, labels = zip(*data)`\n",
        "    \"\"\"\n",
        "\n",
        "    sequences, labels = zip(*data)\n",
        "\n",
        "    y = torch.tensor(labels, dtype=torch.float)\n",
        "    \n",
        "    num_patients = len(sequences)\n",
        "    num_visits = [len(patient) for patient in sequences]\n",
        "    num_codes = [len(visit) for patient in sequences for visit in patient]\n",
        "\n",
        "    max_num_visits = max(num_visits)\n",
        "    max_num_codes = max(num_codes)\n",
        "    \n",
        "    x = torch.zeros((num_patients, max_num_visits, max_num_codes), dtype=torch.long)\n",
        "    rev_x = torch.zeros((num_patients, max_num_visits, max_num_codes), dtype=torch.long)\n",
        "    masks = torch.zeros((num_patients, max_num_visits, max_num_codes), dtype=torch.bool)\n",
        "    rev_masks = torch.zeros((num_patients, max_num_visits, max_num_codes), dtype=torch.bool)\n",
        "    for i_patient, patient in enumerate(sequences):\n",
        "        for j_visit, visit in enumerate(patient):\n",
        "            \"\"\"\n",
        "            TODO: update `x`, `rev_x`, `masks`, and `rev_masks`\n",
        "            \"\"\"\n",
        "            # your code here\n",
        "            raise NotImplementedError\n",
        "    \n",
        "    return x, masks, rev_x, rev_masks, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-12-14T05:55:27.796259Z",
          "start_time": "2021-12-14T05:55:27.784574Z"
        },
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "b8e3661b22fa62f53b537146c05b8cd8",
          "grade": true,
          "grade_id": "cell-4b3472bbf5973793",
          "locked": true,
          "points": 5,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "'''\n",
        "AUTOGRADER CELL. DO NOT MODIFY THIS.\n",
        "'''\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "loader = DataLoader(dataset, batch_size=10, collate_fn=collate_fn)\n",
        "loader_iter = iter(loader)\n",
        "x, masks, rev_x, rev_masks, y = next(loader_iter)\n",
        "\n",
        "assert x.dtype == rev_x.dtype == torch.long\n",
        "assert y.dtype == torch.float\n",
        "assert masks.dtype == rev_masks.dtype == torch.bool\n",
        "\n",
        "assert x.shape == rev_x.shape == masks.shape == rev_masks.shape == (10, 3, 24)\n",
        "assert y.shape == (10,)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "cf11addba53d9041094d45051435cb7e",
          "grade": false,
          "grade_id": "cell-125312ce2d90406a",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "Now we have `CustomDataset` and `collate_fn()`. Let us split the dataset into training and validation sets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-12-14T05:55:27.804195Z",
          "start_time": "2021-12-14T05:55:27.799925Z"
        },
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "cc4a76fda00ecaef5a2e8857a49fb5f2",
          "grade": false,
          "grade_id": "cell-7f2e734b97c94232",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "from torch.utils.data.dataset import random_split\n",
        "\n",
        "split = int(len(dataset)*0.8)\n",
        "\n",
        "lengths = [split, len(dataset) - split]\n",
        "train_dataset, val_dataset = random_split(dataset, lengths)\n",
        "\n",
        "print(\"Length of train dataset:\", len(train_dataset))\n",
        "print(\"Length of val dataset:\", len(val_dataset))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "e8dd53f214983249a91f896697c9e7e4",
          "grade": false,
          "grade_id": "cell-c9732f7be72cb6e4",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "### 1.3 DataLoader [5 points]\n",
        "\n",
        "This is the same as HW3 RNN.\n",
        "\n",
        "Now, we can load the dataset into the data loader."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-12-14T05:55:27.810408Z",
          "start_time": "2021-12-14T05:55:27.806030Z"
        },
        "deletable": false
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "def load_data(train_dataset, val_dataset, collate_fn):\n",
        "    \n",
        "    '''\n",
        "    TODO: Implement this function to return the data loader for  train and validation dataset. \n",
        "    Set batchsize to 32. Set `shuffle=True` only for train dataloader.\n",
        "    \n",
        "    Arguments:\n",
        "        train dataset: train dataset of type `CustomDataset`\n",
        "        val dataset: validation dataset of type `CustomDataset`\n",
        "        collate_fn: collate function\n",
        "        \n",
        "    Outputs:\n",
        "        train_loader, val_loader: train and validation dataloaders\n",
        "    \n",
        "    Note that you need to pass the collate function to the data loader `collate_fn()`.\n",
        "    '''\n",
        "    \n",
        "    batch_size = 32\n",
        "    # your code here\n",
        "    raise NotImplementedError\n",
        "    \n",
        "    return train_loader, val_loader\n",
        "\n",
        "\n",
        "train_loader, val_loader = load_data(train_dataset, val_dataset, collate_fn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-12-14T05:55:27.815520Z",
          "start_time": "2021-12-14T05:55:27.812185Z"
        },
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "d3e8781ea70db7a1afea7413cef8c3cf",
          "grade": true,
          "grade_id": "cell-0c30a49563819f13",
          "locked": true,
          "points": 5,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "'''\n",
        "AUTOGRADER CELL. DO NOT MODIFY THIS.\n",
        "'''\n",
        "\n",
        "train_loader, val_loader = load_data(train_dataset, val_dataset, collate_fn)\n",
        "\n",
        "assert len(train_loader) == 25, \"Length of train_loader should be 25, instead we got %d\"%(len(train_loader))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "761687d5ed78b2d6300e834be8372844",
          "grade": false,
          "grade_id": "cell-a2fa0595b385b366",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "## 2 RETAIN [70 points]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "7e29cad2ba2fdad004bd836333f02a12",
          "grade": false,
          "grade_id": "cell-482be961094244c5",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "RETAIN is essentially a RNN model with attention mechanism.\n",
        " \n",
        "The idea of attention is quite simple: it boils down to weighted averaging. Let us consider machine translation in class as an example. When generating a translation of a source text, we first pass the source text through an encoder (an LSTM or an equivalent model) to obtain a sequence of encoder hidden states $\\boldsymbol{h}_1, \\dots, \\boldsymbol{h}_T$. Then, at each step of generating a translation (decoding), we selectively attend to these encoder hidden states, that is, we construct a context vector $\\boldsymbol{c}_i$ that is a weighted average of encoder hidden states.\n",
        "\n",
        "$$\\boldsymbol{c}_i = \\underset{j}{\\Sigma} a_{ij}\\boldsymbol{h}_j$$\n",
        "\n",
        "We choose the weights $a_{ij}$ based both on encoder hidden states $\\boldsymbol{h}_1, \\dots, \\boldsymbol{h}_T$ and decoder hidden states $\\boldsymbol{s}_1, \\dots, \\boldsymbol{s}_T$ and normalize them so that they encode a categorical probability distribution $p(\\boldsymbol{h}_j | \\boldsymbol{s}_i)$.\n",
        "\n",
        "$$\\boldsymbol{a}_{i} = \\text{Softmax}\\left( a(\\boldsymbol{s}_i, \\boldsymbol{h}_j) \\right)$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "3949fe7f13ff12bcb4b80f0b51a83f10",
          "grade": false,
          "grade_id": "cell-7eb18a6f4e5350fc",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "RETAIN has two different attention mechanisms. \n",
        "- One is to help figure out what are the important visits. This attention $\\alpha_i$, which is scalar for the i-th visit, tells you the importance of the i-th visit.\n",
        "- Then we have another similar attention mechanism. But in this case, this attention ways $\\mathbf{\\beta}_i$ is a vector. That gives us a more detailed view of underlying cause of the input. That is, which are the important features within a visit.\n",
        "\n",
        "<img src=./img/retain-1.png>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-10-22T08:43:41.745655Z",
          "start_time": "2020-10-22T08:43:41.742778Z"
        },
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "946228c26560eb29c953e4c405535a87",
          "grade": false,
          "grade_id": "cell-f982ebdee9832892",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "Unfolded view of RETAIN\u2019s architecture: Given input sequence $\\mathbf{x}_1 , . . . , \\mathbf{x}_i$, we predict the label $\\mathbf{y}_i$. \n",
        "- Step 1: Embedding, \n",
        "- Step 2: generating $\\alpha$ values using RNN-$\\alpha$, \n",
        "- Step 3: generating $\\mathbf{\\beta}$ values using RNN-$\\beta$, \n",
        "- Step 4: Generating the context vector using attention and representation vectors, \n",
        "- Step 5: Making prediction. \n",
        "\n",
        "Note that in Steps 2 and 3 we use RNN in the reversed time.\n",
        "\n",
        "<img src=./img/retain-2.png>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "ce6fa086dbb68cd03fac0325fa85ea44",
          "grade": false,
          "grade_id": "cell-ba7e37a83e467960",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "<img src=./img/retain-3.png>\n",
        "\n",
        "Let us first implement RETAIN step-by-step."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "eb080a7124fccf83a40968d4fb6895e1",
          "grade": false,
          "grade_id": "cell-be223e923ff844ae",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "### 2.1 Step 2: AlphaAttention [20 points]\n",
        "\n",
        "Implement the alpha attention in the second equation of step 2."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-12-14T05:55:27.820629Z",
          "start_time": "2021-12-14T05:55:27.816904Z"
        },
        "deletable": false
      },
      "outputs": [],
      "source": [
        "class AlphaAttention(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, hidden_dim):\n",
        "        super().__init__()\n",
        "        \"\"\"\n",
        "        Define the linear layer `self.a_att` for alpha-attention using `nn.Linear()`;\n",
        "        \n",
        "        Arguments:\n",
        "            hidden_dim: the hidden dimension\n",
        "        \"\"\"\n",
        "        \n",
        "        self.a_att = nn.Linear(hidden_dim, 1)\n",
        "\n",
        "    def forward(self, g):\n",
        "        \"\"\"\n",
        "        TODO: Implement the alpha attention.\n",
        "        \n",
        "        Arguments:\n",
        "            g: the output tensor from RNN-alpha of shape (batch_size, seq_length, hidden_dim) \n",
        "        \n",
        "        Outputs:\n",
        "            alpha: the corresponding attention weights of shape (batch_size, seq_length, 1)\n",
        "            \n",
        "        HINT: consider `torch.softmax`\n",
        "        \"\"\"\n",
        "        \n",
        "        # your code here\n",
        "        raise NotImplementedError"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-12-14T05:55:27.830892Z",
          "start_time": "2021-12-14T05:55:27.822119Z"
        },
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "6189e74151bf8cb193b3eb8f4c8f8cc7",
          "grade": true,
          "grade_id": "cell-6ef630263469161e",
          "locked": true,
          "points": 20,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "'''\n",
        "AUTOGRADER CELL. DO NOT MODIFY THIS.\n",
        "'''\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "70d08cf3cee958b5f0a25e9af3514d2c",
          "grade": false,
          "grade_id": "cell-d09c4bc0d5bf0e78",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "### 2.2 Step 3: BetaAttention [20 points]\n",
        "\n",
        "Implement the beta attention in the second equation of step 3."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-12-14T05:55:27.836205Z",
          "start_time": "2021-12-14T05:55:27.832512Z"
        },
        "deletable": false
      },
      "outputs": [],
      "source": [
        "class BetaAttention(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, hidden_dim):\n",
        "        super().__init__()\n",
        "        \"\"\"\n",
        "        Define the linear layer `self.b_att` for beta-attention using `nn.Linear()`;\n",
        "        \n",
        "        Arguments:\n",
        "            hidden_dim: the hidden dimension\n",
        "        \"\"\"\n",
        "        \n",
        "        self.b_att = nn.Linear(hidden_dim, hidden_dim)\n",
        "\n",
        "\n",
        "    def forward(self, h):\n",
        "        \"\"\"\n",
        "        TODO: Implement the beta attention.\n",
        "        \n",
        "        Arguments:\n",
        "            h: the output tensor from RNN-beta of shape (batch_size, seq_length, hidden_dim) \n",
        "        \n",
        "        Outputs:\n",
        "            beta: the corresponding attention weights of shape (batch_size, seq_length, hidden_dim)\n",
        "            \n",
        "        HINT: consider `torch.tanh`\n",
        "        \"\"\"\n",
        "        \n",
        "        # your code here\n",
        "        raise NotImplementedError"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-12-14T05:55:27.845735Z",
          "start_time": "2021-12-14T05:55:27.837830Z"
        },
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "e748d8021ec4b3a5f120c59ff31af476",
          "grade": true,
          "grade_id": "cell-33b9b75995614f25",
          "locked": true,
          "points": 20,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "'''\n",
        "AUTOGRADER CELL. DO NOT MODIFY THIS.\n",
        "'''\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "f61f81966d7279ef00acc9db4d032b0e",
          "grade": false,
          "grade_id": "cell-92d8f1e9da0e999f",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "### 2.3 Attention Sum [30 points]\n",
        "\n",
        "Implement the sum of attention in step 4."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-12-14T05:55:27.850585Z",
          "start_time": "2021-12-14T05:55:27.847233Z"
        },
        "deletable": false
      },
      "outputs": [],
      "source": [
        "def attention_sum(alpha, beta, rev_v, rev_masks):\n",
        "    \"\"\"\n",
        "    TODO: mask select the hidden states for true visits (not padding visits) and then\n",
        "        sum the them up.\n",
        "\n",
        "    Arguments:\n",
        "        alpha: the alpha attention weights of shape (batch_size, seq_length, 1)\n",
        "        beta: the beta attention weights of shape (batch_size, seq_length, hidden_dim)\n",
        "        rev_v: the visit embeddings in reversed time of shape (batch_size, # visits, embedding_dim)\n",
        "        rev_masks: the padding masks in reversed time of shape (# visits, batch_size, # diagnosis codes)\n",
        "\n",
        "    Outputs:\n",
        "        c: the context vector of shape (batch_size, hidden_dim)\n",
        "        \n",
        "    NOTE: Do NOT use for loop.\n",
        "    \"\"\"\n",
        "    \n",
        "    # your code here\n",
        "    raise NotImplementedError"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-12-14T05:55:27.860495Z",
          "start_time": "2021-12-14T05:55:27.852294Z"
        },
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "2ecb1402ca08327ac62814cb020bc36d",
          "grade": true,
          "grade_id": "cell-832260968193144c",
          "locked": true,
          "points": 30,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "'''\n",
        "AUTOGRADER CELL. DO NOT MODIFY THIS.\n",
        "'''\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "19039e2faa8bc3f96dd2452e5ad874eb",
          "grade": false,
          "grade_id": "cell-e743e3f53f6fa48d",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "### 2.4 Build RETAIN\n",
        "\n",
        "Now, we can build the RETAIN model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-12-14T05:55:27.864879Z",
          "start_time": "2021-12-14T05:55:27.862062Z"
        },
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "657bb0ca386783a293d79ed9ef2b2c2c",
          "grade": false,
          "grade_id": "cell-b6589b3ccf6f9a92",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "def sum_embeddings_with_mask(x, masks):\n",
        "    \"\"\"\n",
        "    Mask select the embeddings for true visits (not padding visits) and then sum the embeddings for each visit up.\n",
        "\n",
        "    Arguments:\n",
        "        x: the embeddings of diagnosis sequence of shape (batch_size, # visits, # diagnosis codes, embedding_dim)\n",
        "        masks: the padding masks of shape (batch_size, # visits, # diagnosis codes)\n",
        "\n",
        "    Outputs:\n",
        "        sum_embeddings: the sum of embeddings of shape (batch_size, # visits, embedding_dim)\n",
        "    \"\"\"\n",
        "    \n",
        "    x = x * masks.unsqueeze(-1)\n",
        "    x = torch.sum(x, dim = -2)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-12-14T05:55:27.882566Z",
          "start_time": "2021-12-14T05:55:27.866531Z"
        },
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "c702d8aa22420dc7dbe58264b6268b9c",
          "grade": false,
          "grade_id": "cell-02ad1b5432de58bd",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "class RETAIN(nn.Module):\n",
        "    \n",
        "    def __init__(self, num_codes, embedding_dim=128):\n",
        "        super().__init__()\n",
        "        # Define the embedding layer using `nn.Embedding`. Set `embDimSize` to 128.\n",
        "        self.embedding = nn.Embedding(num_codes, embedding_dim)\n",
        "        # Define the RNN-alpha using `nn.GRU()`; Set `hidden_size` to 128. Set `batch_first` to True.\n",
        "        self.rnn_a = nn.GRU(embedding_dim, embedding_dim, batch_first=True)\n",
        "        # Define the RNN-beta using `nn.GRU()`; Set `hidden_size` to 128. Set `batch_first` to True.\n",
        "        self.rnn_b = nn.GRU(embedding_dim, embedding_dim, batch_first=True)\n",
        "        # Define the alpha-attention using `AlphaAttention()`;\n",
        "        self.att_a = AlphaAttention(embedding_dim)\n",
        "        # Define the beta-attention using `BetaAttention()`;\n",
        "        self.att_b = BetaAttention(embedding_dim)\n",
        "        # Define the linear layers using `nn.Linear()`;\n",
        "        self.fc = nn.Linear(embedding_dim, 1)\n",
        "        # Define the final activation layer using `nn.Sigmoid().\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "    \n",
        "    def forward(self, x, masks, rev_x, rev_masks):\n",
        "        \"\"\"\n",
        "        Arguments:\n",
        "            rev_x: the diagnosis sequence in reversed time of shape (# visits, batch_size, # diagnosis codes)\n",
        "            rev_masks: the padding masks in reversed time of shape (# visits, batch_size, # diagnosis codes)\n",
        "\n",
        "        Outputs:\n",
        "            probs: probabilities of shape (batch_size)\n",
        "        \"\"\"\n",
        "        # 1. Pass the reversed sequence through the embedding layer;\n",
        "        rev_x = self.embedding(rev_x)\n",
        "        # 2. Sum the reversed embeddings for each diagnosis code up for a visit of a patient.\n",
        "        rev_x = sum_embeddings_with_mask(rev_x, rev_masks)\n",
        "        # 3. Pass the reversed embegginds through the RNN-alpha and RNN-beta layer separately;\n",
        "        g, _ = self.rnn_a(rev_x)\n",
        "        h, _ = self.rnn_b(rev_x)\n",
        "        # 4. Obtain the alpha and beta attentions using `AlphaAttention()` and `BetaAttention()`;\n",
        "        alpha = self.att_a(g)\n",
        "        beta = self.att_b(h)\n",
        "        # 5. Sum the attention up using `attention_sum()`;\n",
        "        c = attention_sum(alpha, beta, rev_x, rev_masks)\n",
        "        # 6. Pass the context vector through the linear and activation layers.\n",
        "        logits = self.fc(c)\n",
        "        probs = self.sigmoid(logits)\n",
        "        return probs.squeeze()\n",
        "    \n",
        "\n",
        "# load the model here\n",
        "retain = RETAIN(num_codes = len(types))\n",
        "retain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-12-14T05:55:27.887114Z",
          "start_time": "2021-12-14T05:55:27.884212Z"
        },
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "5b5f686df90738576e5f392110e51be9",
          "grade": false,
          "grade_id": "cell-aa4410356a92a6fd",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "assert retain.att_a.a_att.in_features == 128, \"alpha attention input features is wrong\"\n",
        "assert retain.att_a.a_att.out_features == 1, \"alpha attention output features is wrong\"\n",
        "assert retain.att_b.b_att.in_features == 128, \"beta attention input features is wrong\"\n",
        "assert retain.att_b.b_att.out_features == 128, \"beta attention output features is wrong\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "8ed184b69aed0279933617da64ae7e19",
          "grade": false,
          "grade_id": "cell-873df7380d762445",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "## 3 Training and Inferencing [10 points]\n",
        "\n",
        "Then, let us implement the `eval()` function first."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-12-14T05:55:28.216859Z",
          "start_time": "2021-12-14T05:55:27.888638Z"
        },
        "deletable": false
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import precision_recall_fscore_support, roc_auc_score\n",
        "\n",
        "\n",
        "def eval(model, val_loader):\n",
        "    \n",
        "    \"\"\"\n",
        "    Evaluate the model.\n",
        "    \n",
        "    Arguments:\n",
        "        model: the RNN model\n",
        "        val_loader: validation dataloader\n",
        "        \n",
        "    Outputs:\n",
        "        precision: overall precision score\n",
        "        recall: overall recall score\n",
        "        f1: overall f1 score\n",
        "        roc_auc: overall roc_auc score\n",
        "        \n",
        "    REFERENCE: checkout https://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics\n",
        "    \"\"\"\n",
        "    \n",
        "    model.eval()\n",
        "    y_pred = torch.LongTensor()\n",
        "    y_score = torch.Tensor()\n",
        "    y_true = torch.LongTensor()\n",
        "    model.eval()\n",
        "    for x, masks, rev_x, rev_masks, y in val_loader:\n",
        "        y_logit = model(x, masks, rev_x, rev_masks)\n",
        "        \"\"\"\n",
        "        TODO: obtain the predicted class (0, 1) by comparing y_logit against 0.5, \n",
        "              assign the predicted class to y_hat.\n",
        "        \"\"\"\n",
        "        y_hat = None\n",
        "        # your code here\n",
        "        raise NotImplementedError\n",
        "        y_score = torch.cat((y_score,  y_logit.detach().to('cpu')), dim=0)\n",
        "        y_pred = torch.cat((y_pred,  y_hat.detach().to('cpu')), dim=0)\n",
        "        y_true = torch.cat((y_true, y.detach().to('cpu')), dim=0)\n",
        "    \n",
        "    p, r, f, _ = precision_recall_fscore_support(y_true, y_pred, average='binary')\n",
        "    roc_auc = roc_auc_score(y_true, y_score)\n",
        "    return p, r, f, roc_auc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "2a52b20a062ffbd7f40dd59951eeb4ff",
          "grade": false,
          "grade_id": "cell-9b3672b70944a8d9",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "Now let us implement the `train()` function. Note that `train()` should call `eval()` at the end of each training epoch to see the results on the validation dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-12-14T05:55:28.223902Z",
          "start_time": "2021-12-14T05:55:28.219022Z"
        },
        "deletable": false
      },
      "outputs": [],
      "source": [
        "def train(model, train_loader, val_loader, n_epochs):\n",
        "    \"\"\"\n",
        "    Train the model.\n",
        "    \n",
        "    Arguments:\n",
        "        model: the RNN model\n",
        "        train_loader: training dataloder\n",
        "        val_loader: validation dataloader\n",
        "        n_epochs: total number of epochs\n",
        "    \"\"\"\n",
        "    \n",
        "    model.train()\n",
        "    for epoch in range(n_epochs):\n",
        "        train_loss = 0\n",
        "        for x, masks, rev_x, rev_masks, y in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            y_hat = model(x, masks, rev_x, rev_masks)\n",
        "            \"\"\" \n",
        "            TODO: calculate the loss using `criterion`, save the output to loss.\n",
        "            \"\"\"\n",
        "            loss = None\n",
        "            # your code here\n",
        "            raise NotImplementedError\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_loss += loss.item()\n",
        "        train_loss = train_loss / len(train_loader)\n",
        "        print('Epoch: {} \\t Training Loss: {:.6f}'.format(epoch+1, train_loss))\n",
        "        p, r, f, roc_auc = eval(model, val_loader)\n",
        "        print('Epoch: {} \\t Validation p: {:.2f}, r:{:.2f}, f: {:.2f}, roc_auc: {:.2f}'.format(epoch+1, p, r, f, roc_auc))\n",
        "    return round(roc_auc, 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-12-14T05:55:30.740539Z",
          "start_time": "2021-12-14T05:55:28.225594Z"
        },
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "0641300bf647c8eecbb44e65fa8ef306",
          "grade": false,
          "grade_id": "cell-2f0785a5f7faf51f",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "# load the model\n",
        "retain = RETAIN(num_codes = len(types))\n",
        "\n",
        "# load the loss function\n",
        "criterion = nn.BCELoss()\n",
        "# load the optimizer\n",
        "optimizer = torch.optim.Adam(retain.parameters(), lr=1e-3)\n",
        "\n",
        "n_epochs = 5\n",
        "train(retain, train_loader, val_loader, n_epochs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-12-14T05:55:30.799227Z",
          "start_time": "2021-12-14T05:55:30.742197Z"
        },
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "2c16be820796ecd899aae374b06d5bcb",
          "grade": true,
          "grade_id": "cell-0f2e27cf698ba97e",
          "locked": true,
          "points": 10,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "'''\n",
        "AUTOGRADER CELL. DO NOT MODIFY THIS.\n",
        "'''\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "58f1139e58a41f00e12f7dc049126b64",
          "grade": false,
          "grade_id": "cell-698c301cbf20c564",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "## 4 Sensitivity analysis [5 points]\n",
        "\n",
        "We will train the same model but with different hyperparameters. We will be using 0.1 and 0.001 for learning rate, and 16, 128 for embedding dimensions. It shows how model performance varies with different values of learning rate and embedding dimensions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-12-14T05:55:38.724727Z",
          "start_time": "2021-12-14T05:55:30.800882Z"
        },
        "deletable": false
      },
      "outputs": [],
      "source": [
        "lr_hyperparameter = [1e-1, 1e-3]\n",
        "embedding_dim_hyperparameter = [8, 128]\n",
        "n_epochs = 5\n",
        "results = {}\n",
        "\n",
        "for lr in lr_hyperparameter:\n",
        "    for embedding_dim in embedding_dim_hyperparameter:\n",
        "        print ('='*50)\n",
        "        print ({'learning rate': lr, \"embedding_dim\": embedding_dim})\n",
        "        print ('-'*50)\n",
        "        \"\"\" \n",
        "        TODO: \n",
        "            1. Load the model by specifying `embedding_dim` as input to RETAIN. It will create different model with different embedding dimension.\n",
        "            2. Load the loss function `nn.BCELoss`\n",
        "            3. Load the optimizer `torch.optim.Adam` with learning rate using `lr` variable\n",
        "        \"\"\"\n",
        "        # your code here\n",
        "        raise NotImplementedError\n",
        "        roc_auc = train(retain, train_loader, val_loader, n_epochs)\n",
        "        results['lr:{},emb:{}'.format(str(lr), str(embedding_dim))] =  roc_auc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-12-14T05:55:38.728372Z",
          "start_time": "2021-12-14T05:55:38.726033Z"
        },
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "c270eaf18a797c7c01b4f40c4a5bfbb3",
          "grade": false,
          "grade_id": "cell-0074e6cb991cea06",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "'''\n",
        "AUTOGRADER CELL. DO NOT MODIFY THIS.\n",
        "'''\n",
        "\n",
        "assert results['lr:0.1,emb:128'] < 0.7, \"auc roc should be below 0.7! Since higher learning rate of 0.1 will not allow the model to converge.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-12-14T05:55:38.736102Z",
          "start_time": "2021-12-14T05:55:38.733354Z"
        },
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "659fefd9bd25ed1913768da00647b816",
          "grade": true,
          "grade_id": "cell-799d937773af6b46",
          "locked": true,
          "points": 5,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "'''\n",
        "AUTOGRADER CELL. DO NOT MODIFY THIS.\n",
        "'''\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "dl4h_mooc_2111",
      "language": "python",
      "name": "dl4h_mooc_2111"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": false,
      "sideBar": true,
      "skip_h1_title": true,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {
        "height": "852px",
        "left": "204px",
        "top": "110px",
        "width": "317.390625px"
      },
      "toc_section_display": true,
      "toc_window_display": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}