{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "4f64a7c4aa8c138f2a216c055f299fb9",
          "grade": false,
          "grade_id": "cell-757f7449dc485bc3",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "# HW1\n",
        "\n",
        "## Overview\n",
        "\n",
        "Preparing the data, computing basic statistics and constructing simple models are essential steps for data science practice. In this homework, you will use clinical data as raw input to perform **Heart Failure Prediction**. For this homework, **Python** programming will be required. See the attached skeleton code as a start-point for the programming questions.\n",
        "\n",
        "This homework assumes familiarity with Pandas. If you need a Pandas crash course, we recommend working through [100 Pandas Puzzles](https://github.com/ajcr/100-pandas-puzzles), the solutions are also available at that link. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "a33f4260efff941e320b24c95157b0e1",
          "grade": false,
          "grade_id": "cell-f4a83de52abb8394",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-17T04:29:59.299540Z",
          "start_time": "2022-01-17T04:29:59.291328Z"
        },
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "17cd37dbc82c1a46157ec9adb81a3844",
          "grade": false,
          "grade_id": "cell-10db081f94b98d02",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "\n",
        "DATA_PATH = \"../HW1-lib/data/\"\n",
        "TRAIN_DATA_PATH = DATA_PATH + \"train/\"\n",
        "VAL_DATA_PATH = DATA_PATH + \"val/\"\n",
        "    \n",
        "sys.path.append(\"../HW1-lib\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "a053ad27ddd1d1d271a6d28c51c634bc",
          "grade": false,
          "grade_id": "cell-52ddaebbf4a162a4",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "85cba0bf235d0a3f169d8bd9cf87350c",
          "grade": false,
          "grade_id": "cell-e59364a32d9b5fc9",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "## About Raw Data\n",
        "\n",
        "For this homework, we will be using a clinical dataset synthesized from [MIMIC-III](https://www.nature.com/articles/sdata201635).\n",
        "\n",
        "Navigate to `TRAIN_DATA_PATH`. There are three CSV files which will be the input data in this homework. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-17T04:29:59.429435Z",
          "start_time": "2022-01-17T04:29:59.302220Z"
        },
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "7fdd12a66d23c25566d9167b74f1f34e",
          "grade": false,
          "grade_id": "cell-d92e9dc996c61070",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "!ls $TRAIN_DATA_PATH"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "46301c39a6360e38c7814cb9f4f1519a",
          "grade": false,
          "grade_id": "cell-d9212ce110d4dc0b",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "**events.csv**\n",
        "\n",
        "The data provided in *events.csv* are event sequences. Each line of this file consists of a tuple with the format *(pid, event_id, vid, value)*. \n",
        "\n",
        "For example, \n",
        "\n",
        "```\n",
        "33,DIAG_244,0,1\n",
        "33,DIAG_414,0,1\n",
        "33,DIAG_427,0,1\n",
        "33,LAB_50971,0,1\n",
        "33,LAB_50931,0,1\n",
        "33,LAB_50812,1,1\n",
        "33,DIAG_425,1,1\n",
        "33,DIAG_427,1,1\n",
        "33,DRUG_0,1,1\n",
        "33,DRUG_3,1,1\n",
        "```\n",
        "\n",
        "- **pid**: De-identified patient identier. For example, the patient in the example above has pid 33. \n",
        "- **event_id**: Clinical event identifier. For example, DIAG_244 means the patient was diagnosed of disease with ICD9 code [244](http://www.icd9data.com/2013/Volume1/240-279/240-246/244/244.htm); LAB_50971 means that the laboratory test with code 50971 was conducted on the patient; and DRUG_0 means that a drug with code 0 was prescribed to the patient. Corresponding lab (drug) names can be found in `{DATA_PATH}/lab_list.txt` (`{DATA_PATH}/drug_list.txt`).\n",
        "- **vid**: Visit identifier. For example, the patient has two visits in total. Note that vid is ordinal. That is, visits with bigger vid occour after that with smaller vid.\n",
        "- **value**: Contains the value associated to an event (always 1 in the synthesized dataset)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "dd71df788c3b463c5db5d09b8991575d",
          "grade": false,
          "grade_id": "cell-b20f2e82052a2926",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "**hf_events.csv**\n",
        "\n",
        "The data provided in *hf_events.csv* contains pid of patients who have been diagnosed with heart failure (i.e., DIAG_398, DIAG_402, DIAG_404, DIAG_428) in at least one visit. They are in the form of a tuple with the format *(pid, vid, label)*. For example,\n",
        "\n",
        "```\n",
        "156,0,1\n",
        "181,1,1\n",
        "```\n",
        "\n",
        "The vid indicates the index of the first visit with heart failure of that patient and a label of 1 indicates the presence of heart failure. **Note that only patients with heart failure are included in this file. Patients who are not mentioned in this file have never been diagnosed with heart failure.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "c5d154237645fcb55795c002c2c56c9f",
          "grade": false,
          "grade_id": "cell-8a8108bebe9e0d39",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "**event_feature_map.csv**\n",
        "\n",
        "The *event_feature_map.csv* is a map from an event_id to an integer index. This file contains *(idx, event_id)* pairs for all event ids."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "4cac1012cd2d5bacf94ba5393cf3ac3b",
          "grade": false,
          "grade_id": "cell-e092836975c6797a",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "## 1 Descriptive Statistics [20 points]\n",
        "\n",
        "Before starting analytic modeling, it is a good practice to get descriptive statistics of the input raw data. In this question, you need to write code that computes various metrics on the data described previously. A skeleton code is provided to you as a starting point.\n",
        "\n",
        "The definition of terms used in the result table are described below:\n",
        "\n",
        "- **Event count**: Number of events recorded for a given patient.\n",
        "- **Encounter count**: Number of visits recorded for a given patient.\n",
        "\n",
        "Note that every line in the input file is an event, while each visit consists of multiple events."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "2c424d35d82a9f9d3dd0fc2dfca71683",
          "grade": false,
          "grade_id": "cell-a53b106557b4e31f",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "**Complete the following code cell to implement the required statistics.**\n",
        "\n",
        "Please be aware that **you are NOT allowed to change the filename and any existing function declarations.** Only `numpy`, `scipy`, `scikit-learn`, `pandas` and other built-in modules of python will be available for you to use. The use of `pandas` library is suggested. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-17T04:29:59.718385Z",
          "start_time": "2022-01-17T04:29:59.432216Z"
        },
        "deletable": false
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import datetime\n",
        "\n",
        "# PLEASE USE THE GIVEN FUNCTION NAME, DO NOT CHANGE IT.\n",
        "\n",
        "def read_csv(filepath=TRAIN_DATA_PATH):\n",
        "\n",
        "    '''\n",
        "    Read the events.csv and hf_events.csv files. \n",
        "    Variables returned from this function are passed as input to the metric functions.\n",
        "    \n",
        "    NOTE: remember to use `filepath` whose default value is `TRAIN_DATA_PATH`.\n",
        "    '''\n",
        "    \n",
        "    events = pd.read_csv(filepath + 'events.csv')\n",
        "    hf = pd.read_csv(filepath + 'hf_events.csv')\n",
        "\n",
        "    return events, hf\n",
        "\n",
        "def event_count_metrics(events, hf):\n",
        "\n",
        "    '''\n",
        "    TODO : Implement this function to return the event count metrics.\n",
        "    \n",
        "    Event count is defined as the number of events recorded for a given patient.\n",
        "    '''\n",
        "    \n",
        "    avg_hf_event_count = None\n",
        "    max_hf_event_count = None\n",
        "    min_hf_event_count = None\n",
        "    avg_norm_event_count = None\n",
        "    max_norm_event_count = None\n",
        "    min_norm_event_count = None\n",
        "    \n",
        "    # your code here\n",
        "    raise NotImplementedError\n",
        "\n",
        "    return avg_hf_event_count, max_hf_event_count, min_hf_event_count, \\\n",
        "           avg_norm_event_count, max_norm_event_count, min_norm_event_count\n",
        "\n",
        "def encounter_count_metrics(events, hf):\n",
        "\n",
        "    '''\n",
        "    TODO : Implement this function to return the encounter count metrics.\n",
        "    \n",
        "    Encounter count is defined as the number of visits recorded for a given patient. \n",
        "    '''\n",
        "    \n",
        "    avg_hf_encounter_count = None\n",
        "    max_hf_encounter_count = None\n",
        "    min_hf_encounter_count = None\n",
        "    avg_norm_encounter_count = None\n",
        "    max_norm_encounter_count = None\n",
        "    min_norm_encounter_count = None\n",
        "    \n",
        "    # your code here\n",
        "    raise NotImplementedError\n",
        "\n",
        "    return avg_hf_encounter_count, max_hf_encounter_count, min_hf_encounter_count, \\\n",
        "           avg_norm_encounter_count, max_norm_encounter_count, min_norm_encounter_count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-17T04:30:00.227147Z",
          "start_time": "2022-01-17T04:29:59.720539Z"
        },
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "9e7427796cf3b4d5a493bd096d082ac9",
          "grade": false,
          "grade_id": "cell-d28045e592b8f081",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "'''\n",
        "DO NOT MODIFY THIS.\n",
        "'''\n",
        "\n",
        "events, hf = read_csv(TRAIN_DATA_PATH)\n",
        "\n",
        "#Compute the event count metrics\n",
        "start_time = time.time()\n",
        "event_count = event_count_metrics(events, hf)\n",
        "end_time = time.time()\n",
        "print((\"Time to compute event count metrics: \" + str(end_time - start_time) + \"s\"))\n",
        "print(event_count)\n",
        "\n",
        "#Compute the encounter count metrics\n",
        "start_time = time.time()\n",
        "encounter_count = encounter_count_metrics(events, hf)\n",
        "end_time = time.time()\n",
        "print((\"Time to compute encounter count metrics: \" + str(end_time - start_time) + \"s\"))\n",
        "print(encounter_count)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-17T04:30:00.559437Z",
          "start_time": "2022-01-17T04:30:00.229054Z"
        },
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "6d140e954c3a28d1e5479935f3ef713e",
          "grade": true,
          "grade_id": "cell-8f9c85ba731d96e5",
          "locked": true,
          "points": 10,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "'''\n",
        "AUTOGRADER CELL. DO NOT MODIFY THIS.\n",
        "'''\n",
        "\n",
        "events, hf = read_csv(TRAIN_DATA_PATH)\n",
        "event_count = event_count_metrics(events, hf)\n",
        "assert event_count == (188.9375, 2046, 28, 118.64423076923077, 1014, 6), \"event_count failed!\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-17T04:30:01.151817Z",
          "start_time": "2022-01-17T04:30:00.561593Z"
        },
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "557341b6198f8525680acb1753cbbdbf",
          "grade": true,
          "grade_id": "cell-343b745247b90367",
          "locked": true,
          "points": 10,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "'''\n",
        "AUTOGRADER CELL. DO NOT MODIFY THIS.\n",
        "'''\n",
        "\n",
        "events, hf = read_csv(TRAIN_DATA_PATH)\n",
        "encounter_count = encounter_count_metrics(events, hf)\n",
        "assert encounter_count == (2.8060810810810812, 34, 2, 2.189423076923077, 11, 1), \"encounter_count failed!\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "d67dbea2960c373fa33683228f344817",
          "grade": false,
          "grade_id": "cell-c5413d29c37a0a33",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "## 2 Feature construction [40 points] \n",
        "\n",
        "It is a common practice to convert raw data into a standard data format before running real machine learning models. In this question, you will implement the necessary python functions in this script. You will work with *events.csv*, *hf_events.csv* and *event_feature_map.csv* files provided in **TRAIN_DATA_PATH** folder. The use of `pandas` library in this question is recommended. \n",
        "\n",
        "Listed below are a few concepts you need to know before beginning feature construction (for details please refer to lectures). \n",
        "\n",
        "<img src=\"img/window.jpg\" width=\"600\"/>\n",
        "\n",
        "- **Index vid**: Index vid is evaluated as follows:\n",
        "  - For heart failure patients: Index vid is the vid of the first visit with heart failure for that patient (i.e., vid field in *hf_events.csv*). \n",
        "  - For normal patients: Index vid is the vid of the last visit for that patient (i.e., vid field in *events.csv*). \n",
        "- **Observation Window**: The time interval you will use to identify relevant events. Only events present in this window should be included while constructing feature vectors.\n",
        "- **Prediction Window**: A fixed time interval that is to be used to make the prediction.\n",
        "\n",
        "In the example above, the index vid is 3. Visits with vid 0, 1, 2 are within the observation window. The prediction window is between visit 2 and 3."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "e0a8860140e64060ac50e4ca3c701ea1",
          "grade": false,
          "grade_id": "cell-aa768bbdeed84907",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "### 2.1 Compute the index vid [10 points]\n",
        "\n",
        "Use the definition provided above to compute the index vid for all patients. Complete the method `read_csv` and `calculate_index_vid` provided in the following code cell. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-17T04:30:01.160227Z",
          "start_time": "2022-01-17T04:30:01.154297Z"
        },
        "deletable": false
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import datetime\n",
        "\n",
        "\n",
        "def read_csv(filepath=TRAIN_DATA_PATH):\n",
        "    \n",
        "    '''\n",
        "    Read the events.csv, hf_events.csv and event_feature_map.csv files.\n",
        "    \n",
        "    NOTE: remember to use `filepath` whose default value is `TRAIN_DATA_PATH`.\n",
        "    '''\n",
        "\n",
        "    events = pd.read_csv(filepath + 'events.csv')\n",
        "    hf = pd.read_csv(filepath + 'hf_events.csv')\n",
        "    feature_map = pd.read_csv(filepath + 'event_feature_map.csv')\n",
        "\n",
        "    return events, hf, feature_map\n",
        "\n",
        "\n",
        "def calculate_index_vid(events, hf):\n",
        "    \n",
        "    '''\n",
        "    TODO: This function needs to be completed.\n",
        "\n",
        "    Suggested steps:\n",
        "        1. Create list of normal patients (hf_events.csv only contains information about heart failure patients).\n",
        "        2. Split events into two groups based on whether the patient has heart failure or not.\n",
        "        3. Calculate index vid for each patient.\n",
        "    \n",
        "    IMPORTANT:\n",
        "        `indx_vid` should be a pd dataframe with header `['pid', 'indx_vid']`.\n",
        "    '''\n",
        "\n",
        "    indx_vid = ''\n",
        "    \n",
        "    # your code here\n",
        "    raise NotImplementedError\n",
        "    \n",
        "    return indx_vid"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-17T04:30:01.464338Z",
          "start_time": "2022-01-17T04:30:01.162278Z"
        },
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "12811f08d77cb33b2b8821e3da47d7c9",
          "grade": true,
          "grade_id": "cell-7207b1c9d6f08a03",
          "locked": true,
          "points": 10,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "'''\n",
        "AUTOGRADER CELL. DO NOT MODIFY THIS.\n",
        "'''\n",
        "\n",
        "events, hf, feature_map = read_csv(TRAIN_DATA_PATH)\n",
        "indx_vid_df = calculate_index_vid(events, hf)\n",
        "assert indx_vid_df.shape == (4000, 2), \"calculate_index_vid failed!\"\n",
        "\n",
        "indx_vid = dict(list(zip(indx_vid_df.pid, indx_vid_df.indx_vid)))\n",
        "assert indx_vid[78] == 1, \"calculate_index_vid failed!\"\n",
        "assert indx_vid[1230] == 5, \"calculate_index_vid failed!\"\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "c2bba556d9b1940fed3518e2f0b90e05",
          "grade": false,
          "grade_id": "cell-4a3b0c07c74dc8dc",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "### 2.2 Filter events [10 points]\n",
        "\n",
        "Remove the events that occur outside the observation window. That is, all events in visits before index vid. Complete the method *filter_events* provided in the following code cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-17T04:30:01.469360Z",
          "start_time": "2022-01-17T04:30:01.465690Z"
        },
        "deletable": false
      },
      "outputs": [],
      "source": [
        "def filter_events(events, indx_vid):\n",
        "    \n",
        "    '''\n",
        "    TODO: This function needs to be completed.\n",
        "\n",
        "    Suggested steps:\n",
        "        1. Join indx_vid with events on pid.\n",
        "        2. Filter events occuring in the observation window [:, index vid) (Note that the right side is OPEN).\n",
        "    \n",
        "    \n",
        "    IMPORTANT:\n",
        "        `filtered_events` should be a pd dataframe withe header  `['pid', 'event_id', 'value']`.\n",
        "    '''\n",
        "\n",
        "    filtered_events = None\n",
        "    \n",
        "    # your code here\n",
        "    raise NotImplementedError\n",
        "    \n",
        "    return filtered_events"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-17T04:30:02.026928Z",
          "start_time": "2022-01-17T04:30:01.472907Z"
        },
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "a72ba59ec4465e07159f63371dcaaeca",
          "grade": true,
          "grade_id": "cell-a298eb0fe4ed28f9",
          "locked": true,
          "points": 10,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "'''\n",
        "AUTOGRADER CELL. DO NOT MODIFY THIS.\n",
        "'''\n",
        "\n",
        "events, hf, feature_map = read_csv(TRAIN_DATA_PATH)\n",
        "indx_vid = calculate_index_vid(events, hf)\n",
        "filtered_events = filter_events(events, indx_vid)\n",
        "assert filtered_events[filtered_events.pid == 78].shape == (128, 3), \"filter_events failed!\"\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "a74bda5c299d66e1901820f18bb9aa25",
          "grade": false,
          "grade_id": "cell-845f881f710c6b3b",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "### 2.3 Aggregate events [10 points]\n",
        "\n",
        "To create features suitable for machine learning, we will need to aggregate the events for each patient as follows:\n",
        "\n",
        "- **count** occurences for each event.\n",
        "\n",
        "Each event type will become a feature and we will directly use event_id as feature name. For example, given below raw event sequence for a patient,\n",
        "\n",
        "```\n",
        "33,DIAG_244,0,1\n",
        "33,LAB_50971,0,1\n",
        "33,LAB_50931,0,1\n",
        "33,LAB_50931,0,1\n",
        "33,DIAG_244,1,1\n",
        "33,DIAG_427,1,1\n",
        "33,DRUG_0,1,1\n",
        "33,DRUG_3,1,1\n",
        "33,DRUG_3,1,1\n",
        "```\n",
        "\n",
        "We can get feature value pairs *(event_id, value)* for this patient with ID *33* as\n",
        "```\n",
        "(DIAG_244, 2.0)\n",
        "(LAB_50971, 1.0)\n",
        "(LAB_50931, 2.0)\n",
        "(DIAG_427, 1.0)\n",
        "(DRUG_0, 1.0)\n",
        "(DRUG_3, 2.0)\n",
        "```\n",
        "\n",
        "Next, replace each *event_id* with the *feature_id* provided in *event_feature_map.csv*.\n",
        "\n",
        "```\n",
        "(146, 2.0)\n",
        "(1434, 1.0)\n",
        "(1429, 2.0)\n",
        "(304, 1.0)\n",
        "(898, 1.0)\n",
        "(1119, 2.0)\n",
        "```\n",
        "\n",
        "Lastly, in machine learning algorithm like logistic regression, it is important to normalize different features into the same scale. We will use the [min-max normalization](http://stats.stackexchange.com/questions/70801/how-to-normalize-data-to-0-1-range) approach. (Note: we define $min(x)$ is always 0, i.e. the scale equation become $x$/$max(x)$).\n",
        "\n",
        "Complete the method *aggregate_events* provided in the following code cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-17T04:30:02.033863Z",
          "start_time": "2022-01-17T04:30:02.028268Z"
        },
        "deletable": false
      },
      "outputs": [],
      "source": [
        "def aggregate_events(filtered_events_df, hf_df, feature_map_df):\n",
        "    \n",
        "    '''\n",
        "    TODO: This function needs to be completed.\n",
        "\n",
        "    Suggested steps:\n",
        "        1. Replace event_id's with index available in event_feature_map.csv.\n",
        "        2. Aggregate events using count to calculate feature value.\n",
        "        3. Normalize the values obtained above using min-max normalization(the min value will be 0 in all scenarios).\n",
        "    \n",
        "    \n",
        "    IMPORTANT:\n",
        "        `aggregated_events` should be a pd dataframe with header `['pid', 'feature_id', 'feature_value']`.\n",
        "    '''\n",
        "    \n",
        "    aggregated_events = None\n",
        "    \n",
        "    # your code here\n",
        "    raise NotImplementedError\n",
        "    \n",
        "    return aggregated_events"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-17T04:30:02.786817Z",
          "start_time": "2022-01-17T04:30:02.035456Z"
        },
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "f36119f0574d09e596cc891bd1f71bd4",
          "grade": true,
          "grade_id": "cell-b5931f772d6dac9f",
          "locked": true,
          "points": 10,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "'''\n",
        "AUTOGRADER CELL. DO NOT MODIFY THIS.\n",
        "'''\n",
        "\n",
        "events, hf, feature_map = read_csv(TRAIN_DATA_PATH)\n",
        "index_vid = calculate_index_vid(events, hf)\n",
        "filtered_events = filter_events(events, index_vid)\n",
        "aggregated_events = aggregate_events(filtered_events, hf, feature_map)\n",
        "assert aggregated_events[aggregated_events.pid == 88037].shape == (29, 3), \"aggregate_events failed!\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "57aa824b8fd9cf75ae7da47f0ff3156b",
          "grade": false,
          "grade_id": "cell-8db3e08c181b90cc",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "### 2.4 Save in  SVMLight format [10 points]\n",
        "\n",
        "If the dimensionality of a feature vector is large but the feature vector is sparse (i.e. it has only a few nonzero elements), sparse representation should be employed. In this problem you will use the provided data for each patient to construct a feature vector and represent the feature vector in SVMLight format.\n",
        "\n",
        "```\n",
        "<line> .=. <target> <feature>:<value> <feature>:<value>\n",
        "<target> .=. 1 | 0\n",
        "<feature> .=. <integer>\n",
        "<value> .=. <float>\n",
        "```\n",
        "\n",
        "The target value and each of the feature/value pairs are separated by a space character. Feature/value pairs MUST be ordered by increasing feature number. **(Please do this in `save_svmlight()`.)** Features with value zero can be skipped. For example, the feature vector in SVMLight format will look like: \n",
        "\n",
        "```\n",
        "1 2:0.5 3:0.12 10:0.9 2000:0.3\n",
        "0 4:1.0 78:0.6 1009:0.2\n",
        "1 33:0.1 34:0.98 1000:0.8 3300:0.2\n",
        "1 34:0.1 389:0.32\n",
        "```\n",
        "\n",
        "where, 1 or 0 will indicate whether the patient has heart failure or not (i.e. the label) and it will be followed by a series of feature-value pairs **sorted** by the feature index (idx) value.\n",
        "\n",
        "You may find *utils.py* useful. You can review the code by running `%load utils.py`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-17T04:30:02.790053Z",
          "start_time": "2022-01-17T04:30:02.788376Z"
        },
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "5615560ca2a7bb2e7ce7134dedb800ab",
          "grade": false,
          "grade_id": "cell-286e10c42da874be",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "# %load   ../HW1-lib/utils.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-17T04:30:03.114846Z",
          "start_time": "2022-01-17T04:30:02.791439Z"
        },
        "deletable": false
      },
      "outputs": [],
      "source": [
        "import utils\n",
        "import collections\n",
        "\n",
        "def create_features(events_in, hf_in, feature_map_in):\n",
        "\n",
        "    indx_vid = calculate_index_vid(events_in, hf_in)\n",
        "\n",
        "    #Filter events in the observation window\n",
        "    filtered_events = filter_events(events_in, indx_vid)\n",
        "\n",
        "    #Aggregate the event values for each patient \n",
        "    aggregated_events = aggregate_events(filtered_events, hf_in, feature_map_in)\n",
        "\n",
        "    '''\n",
        "    TODO: Complete the code below by creating two dictionaries.\n",
        "        1. patient_features : Key is pid and value is array of tuples(feature_id, feature_value). \n",
        "                              Note that pid should be integer.\n",
        "        2. hf : Key is pid and value is heart failure label.\n",
        "    '''\n",
        "    patient_features = None\n",
        "    hf = None\n",
        "    \n",
        "    # your code here\n",
        "    raise NotImplementedError\n",
        "\n",
        "    return patient_features, hf\n",
        "\n",
        "def save_svmlight(patient_features, hf, op_file):\n",
        "    \n",
        "    '''\n",
        "    TODO: This function needs to be completed.\n",
        "\n",
        "    Create op_file: - which saves the features in svmlight format. (See instructions in section 2.4 for detailed explanatiom)\n",
        "    \n",
        "    Note: Please make sure the features are ordered in ascending order, and patients are stored in ascending order as well.     \n",
        "    To save the files, you could write:\n",
        "        deliverable.write(bytes(f\"{label} {feature_value} \\n\", 'utf-8'))\n",
        "    '''\n",
        "    \n",
        "    deliverable = open(op_file, 'wb')\n",
        "    # your code here\n",
        "    raise NotImplementedError"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-17T04:30:03.318468Z",
          "start_time": "2022-01-17T04:30:03.115935Z"
        },
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "f36390a5de4e9e4ed4395f8cef946bd7",
          "grade": true,
          "grade_id": "cell-5f208e4ea6e5a154",
          "locked": true,
          "points": 10,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "'''\n",
        "AUTOGRADER CELL. DO NOT MODIFY THIS.\n",
        "'''\n",
        "\n",
        "events_in, hf_in, feature_map_in = read_csv(TRAIN_DATA_PATH)\n",
        "events_in = events_in.loc[:1000]\n",
        "hf_in = hf_in.loc[:100]\n",
        "patient_features, hf = create_features(events_in, hf_in, feature_map_in)\n",
        "assert 78 in patient_features, \"create_features is missing patients\"\n",
        "assert len(patient_features[78]) == 127, \"create_features is wrong\"\n",
        "assert patient_features[78][:5] == [(20, 1.0), (164, 1.0), (175, 1.0), (182, 1.0), (190, 1.0)], \"create_features is wrong\"\n",
        "assert len(hf) == 101, \"create_features is wrong\"\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "cce7c0a2c651e4e8cddad28f5f0dbc0e",
          "grade": false,
          "grade_id": "cell-480980a849ebd089",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "The whole pipeline:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-17T04:30:18.478851Z",
          "start_time": "2022-01-17T04:30:03.320257Z"
        },
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "d511eacce421e41b3f8e0a5d0e69639a",
          "grade": false,
          "grade_id": "cell-5be75ccfce6fe003",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "def main():\n",
        "    events_in, hf_in, feature_map_in = read_csv(TRAIN_DATA_PATH)\n",
        "    patient_features, hf = create_features(events_in, hf_in, feature_map_in)\n",
        "    save_svmlight(patient_features, hf, 'features_svmlight.train')\n",
        "    \n",
        "    events_in, hf_in, feature_map_in = read_csv(VAL_DATA_PATH)\n",
        "    patient_features, hf = create_features(events_in, hf_in, feature_map_in)\n",
        "    save_svmlight(patient_features, hf, 'features_svmlight.val')\n",
        "    \n",
        "main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "c28533d723309779b8530cb354815f28",
          "grade": false,
          "grade_id": "cell-a02bc53e8da63b0b",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "## 3 Predictive Modeling [40 points]\n",
        "\n",
        "Make sure you have finished section 2 before you start to work on this question because some of the files generated in section 2 (*features_svmlight.train*) will be used in this question."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "94422b239845574543cc7842983e0b86",
          "grade": false,
          "grade_id": "cell-87c0e61ad837e42c",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "### 3.1 Model Creation [20 points]\n",
        "\n",
        "In the previous question, you constructed feature vectors for patients to be used as training data in various predictive models (classifiers). Now you will use this training data (*features_svmlight.train*) in 3 predictive models. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "9cc834d3630587624b8a4bcfe680de97",
          "grade": false,
          "grade_id": "cell-4c04be8133e4b75f",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "**Step - a. Implement Logistic Regression, SVM and Decision Tree. Skeleton code is provided in the following code cell.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-17T04:30:18.840336Z",
          "start_time": "2022-01-17T04:30:18.480730Z"
        },
        "deletable": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import load_svmlight_file\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import *\n",
        "\n",
        "import utils\n",
        "\n",
        "\n",
        "# PLEASE USE THE GIVEN FUNCTION NAME, DO NOT CHANGE IT.\n",
        "# USE THIS RANDOM STATE FOR ALL OF THE PREDICTIVE MODELS.\n",
        "# OR THE TESTS WILL NEVER PASS.\n",
        "RANDOM_STATE = 545510477\n",
        "\n",
        "\n",
        "#input: X_train, Y_train\n",
        "#output: Y_pred\n",
        "def logistic_regression_pred(X_train, Y_train):\n",
        "    \n",
        "    \"\"\"\n",
        "    TODO: Train a logistic regression classifier using X_train and Y_train.\n",
        "    Use this to predict labels of X_train. Use default params for the classifier.\n",
        "    \"\"\"\n",
        "    \n",
        "    # your code here\n",
        "    raise NotImplementedError\n",
        "\n",
        "    \n",
        "#input: X_train, Y_train\n",
        "#output: Y_pred\n",
        "def svm_pred(X_train, Y_train):\n",
        "    \n",
        "    \"\"\"\n",
        "    TODO: Train a SVM classifier using X_train and Y_train.\n",
        "    Use this to predict labels of X_train. Use default params for the classifier\n",
        "    \"\"\"\n",
        "    \n",
        "    # your code here\n",
        "    raise NotImplementedError\n",
        "\n",
        "    \n",
        "#input: X_train, Y_train\n",
        "#output: Y_pred\n",
        "def decisionTree_pred(X_train, Y_train):\n",
        "\n",
        "    \"\"\"\n",
        "    TODO: Train a logistic regression classifier using X_train and Y_train.\n",
        "    Use this to predict labels of X_train. Use max_depth as 5.\n",
        "    \"\"\"\n",
        "    \n",
        "    # your code here\n",
        "    raise NotImplementedError\n",
        "\n",
        "    \n",
        "#input: Y_pred,Y_true\n",
        "#output: accuracy, precision, recall, f1-score\n",
        "def classification_metrics(Y_pred, Y_true):\n",
        "    \n",
        "    \"\"\"\n",
        "    TODO: Calculate the above mentioned metrics.\n",
        "    NOTE: It is important to provide the output in the same order.\n",
        "    \"\"\"\n",
        "    \n",
        "    # your code here\n",
        "    raise NotImplementedError\n",
        "\n",
        "    \n",
        "#input: Name of classifier, predicted labels, actual labels\n",
        "def display_metrics(classifierName, Y_pred, Y_true):\n",
        "    print(\"______________________________________________\")\n",
        "    print((\"Classifier: \"+classifierName))\n",
        "    acc, precision, recall, f1score = classification_metrics(Y_pred,Y_true)\n",
        "    print((\"Accuracy: \"+str(acc)))\n",
        "    print((\"Precision: \"+str(precision)))\n",
        "    print((\"Recall: \"+str(recall)))\n",
        "    print((\"F1-score: \"+str(f1score)))\n",
        "    print(\"______________________________________________\")\n",
        "    print(\"\")\n",
        "\n",
        "    \n",
        "def main():\n",
        "    X_train, Y_train = utils.get_data_from_svmlight(\"features_svmlight.train\")\n",
        "\n",
        "    display_metrics(\"Logistic Regression\", logistic_regression_pred(X_train, Y_train), Y_train)\n",
        "    display_metrics(\"SVM\",svm_pred(X_train, Y_train),Y_train)\n",
        "    display_metrics(\"Decision Tree\", decisionTree_pred(X_train, Y_train), Y_train)\n",
        "\n",
        "    \n",
        "main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-17T04:30:19.121904Z",
          "start_time": "2022-01-17T04:30:18.842373Z"
        },
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "034aed31d22576811319cd685d665149",
          "grade": true,
          "grade_id": "cell-c59bbb34681f832b",
          "locked": true,
          "points": 10,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "'''\n",
        "AUTOGRADER CELL. DO NOT MODIFY THIS.\n",
        "'''\n",
        "\n",
        "from utils import get_data_from_svmlight\n",
        "from numpy.testing import assert_almost_equal\n",
        "\n",
        "### 3.1a Training Accuracy [3 points]\n",
        "X_train, Y_train = get_data_from_svmlight(\"features_svmlight.train\")\n",
        "\n",
        "# test_accuracy_lr\n",
        "expected = 0.856338028169014\n",
        "Y_pred = logistic_regression_pred(X_train, Y_train)\n",
        "actual = classification_metrics(Y_pred, Y_train)[0]\n",
        "assert_almost_equal(actual, expected, decimal=2, verbose=False, err_msg=\"test_accuracy_lr failed!\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "69dd38d29d3e7c0035782d6569b16c3b",
          "grade": false,
          "grade_id": "cell-f6628dbab9bc57d4",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "**Step - b. Evaluate your predictive models on a separate test dataset in *features_svmlight.val* (binary labels are provided in that svmlight file as the first field). Skeleton code is provided in the following code cell.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-17T04:30:19.404126Z",
          "start_time": "2022-01-17T04:30:19.123750Z"
        },
        "deletable": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import load_svmlight_file\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import *\n",
        "\n",
        "import utils\n",
        "\n",
        "\n",
        "# PLEASE USE THE GIVEN FUNCTION NAME, DO NOT CHANGE IT.\n",
        "# USE THIS RANDOM STATE FOR ALL OF THE PREDICTIVE MODELS.\n",
        "# OR THE TESTS WILL NEVER PASS.\n",
        "RANDOM_STATE = 545510477\n",
        "\n",
        "\n",
        "#input: X_train, Y_train and X_test\n",
        "#output: Y_pred\n",
        "def logistic_regression_pred(X_train, Y_train, X_test):\n",
        "\n",
        "    \"\"\"\n",
        "    TODO: train a logistic regression classifier using X_train and Y_train. \n",
        "    Use this to predict labels of X_test use default params for the classifier.\n",
        "    \"\"\"\n",
        "    \n",
        "    # your code here\n",
        "    raise NotImplementedError\n",
        "    \n",
        "\n",
        "#input: X_train, Y_train and X_test\n",
        "#output: Y_pred\n",
        "def svm_pred(X_train, Y_train, X_test):\n",
        "    \n",
        "    \"\"\"\n",
        "    TODO: Train a SVM classifier using X_train and Y_train.\n",
        "    Use this to predict labels of X_test use default params for the classifier.\n",
        "    \"\"\"\n",
        "    \n",
        "    # your code here\n",
        "    raise NotImplementedError\n",
        "\n",
        "    \n",
        "#input: X_train, Y_train and X_test\n",
        "#output: Y_pred\n",
        "def decisionTree_pred(X_train, Y_train, X_test):\n",
        "    \n",
        "    \"\"\"\n",
        "    TODO: Train a logistic regression classifier using X_train and Y_train.\n",
        "    Use this to predict labels of X_test.\n",
        "    IMPORTANT: use max_depth as 5. Else your test cases might fail.\n",
        "    \"\"\"\n",
        "    \n",
        "    # your code here\n",
        "    raise NotImplementedError\n",
        "\n",
        "\n",
        "#input: Y_pred,Y_true\n",
        "#output: accuracy, precision, recall, f1-score\n",
        "def classification_metrics(Y_pred, Y_true):\n",
        "    \n",
        "    \"\"\"\n",
        "    TODO: Calculate the above mentioned metrics.\n",
        "    NOTE: It is important to provide the output in the same order.\n",
        "    \"\"\"\n",
        "    \n",
        "    # your code here\n",
        "    raise NotImplementedError\n",
        "\n",
        "    \n",
        "#input: Name of classifier, predicted labels, actual labels\n",
        "def display_metrics(classifierName, Y_pred, Y_true):\n",
        "    print(\"______________________________________________\")\n",
        "    print((\"Classifier: \"+classifierName))\n",
        "    acc, precision, recall, f1score = classification_metrics(Y_pred,Y_true)\n",
        "    print((\"Accuracy: \"+str(acc)))\n",
        "    print((\"Precision: \"+str(precision)))\n",
        "    print((\"Recall: \"+str(recall)))\n",
        "    print((\"F1-score: \"+str(f1score)))\n",
        "    print(\"______________________________________________\")\n",
        "    print(\"\")\n",
        "\n",
        "    \n",
        "def main():\n",
        "    X_train, Y_train = utils.get_data_from_svmlight(\"features_svmlight.train\")\n",
        "    X_test, Y_test = utils.get_data_from_svmlight(os.path.join(\"features_svmlight.val\"))\n",
        "\n",
        "    display_metrics(\"Logistic Regression\", logistic_regression_pred(X_train, Y_train, X_test), Y_test)\n",
        "    display_metrics(\"SVM\", svm_pred(X_train, Y_train, X_test), Y_test)\n",
        "    display_metrics(\"Decision Tree\", decisionTree_pred(X_train, Y_train, X_test), Y_test)\n",
        "\n",
        "\n",
        "main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-17T04:30:19.677389Z",
          "start_time": "2022-01-17T04:30:19.406083Z"
        },
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "b37be2e239bb84e1dbe3df96764e934e",
          "grade": true,
          "grade_id": "cell-f23ffe89a0cd304a",
          "locked": true,
          "points": 10,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "'''\n",
        "AUTOGRADER CELL. DO NOT MODIFY THIS.\n",
        "'''\n",
        "\n",
        "from utils import get_data_from_svmlight\n",
        "from numpy.testing import assert_almost_equal\n",
        "\n",
        "### 3.1b Prediction Accuracy [3 points]\n",
        "X_train, Y_train = get_data_from_svmlight(\"features_svmlight.train\")\n",
        "X_test, Y_test = get_data_from_svmlight(\"features_svmlight.val\")\n",
        "\n",
        "# test_accuracy_lr\n",
        "expected = 0.6937086092715232\n",
        "Y_pred = logistic_regression_pred(X_train, Y_train, X_test)\n",
        "actual = classification_metrics(Y_pred, Y_test)[0]\n",
        "assert_almost_equal(actual, expected, decimal=2, verbose=False, err_msg=\"test_accuracy_lr failed!\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "f417499e53d12d286cd07c58cb33130e",
          "grade": false,
          "grade_id": "cell-1a6ab34b3605550e",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "### 3.2 Model Validation [20 points]\n",
        "\n",
        "In order to fully utilize the available data and obtain more reliable results, machine learning practitioners use cross-validation to evaluate and improve their predictive models. You will demonstrate using two cross-validation strategies against SVD. \n",
        "\n",
        "- K-fold: Divide all the data into $k$ groups of samples. Each time $\\frac{1}{k}$ samples will be used as test data and the remaining samples as training data.\n",
        "- Randomized K-fold: Iteratively random shuffle the whole dataset and use top specific percentage of data as training and the rest as test. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "6a87293971a18439d3a0aabf105fa235",
          "grade": false,
          "grade_id": "cell-795340da3ac71dd4",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "**Implement the two cross-validation strategies.**\n",
        "- **K-fold:** Use the number of iterations k=5; \n",
        "- **Randomized K-fold**: Use a test data percentage of 20\\% and k=5 for the number of iterations for Randomized"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-17T04:30:20.379732Z",
          "start_time": "2022-01-17T04:30:19.679182Z"
        },
        "deletable": false
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import KFold, ShuffleSplit\n",
        "from numpy import mean\n",
        "\n",
        "import utils\n",
        "\n",
        "\n",
        "# PLEASE USE THE GIVEN FUNCTION NAME, DO NOT CHANGE IT.\n",
        "# USE THIS RANDOM STATE FOR ALL OF THE PREDICTIVE MODELS.\n",
        "# OR THE TESTS WILL NEVER PASS.\n",
        "RANDOM_STATE = 545510477\n",
        "\n",
        "\n",
        "#input: training data and corresponding labels\n",
        "#output: accuracy, f1\n",
        "def get_f1_kfold(X, Y, k=5):\n",
        "    \n",
        "    \"\"\"\n",
        "    TODO: First get the train indices and test indices for each iteration.\n",
        "    Then train the classifier accordingly.\n",
        "    Report the mean f1 score of all the folds.\n",
        "    \n",
        "    Note that you do not need to set random_state for KFold, as it has no effect since shuffle is False by default.\n",
        "    \"\"\"\n",
        "    \n",
        "    # your code here\n",
        "    raise NotImplementedError\n",
        "\n",
        "    \n",
        "#input: training data and corresponding labels\n",
        "#output: accuracy, f1\n",
        "def get_f1_randomisedCV(X, Y, iterNo=5, test_percent=0.20):\n",
        "\n",
        "    \"\"\"\n",
        "    TODO: First get the train indices and test indices for each iteration.\n",
        "    Then train the classifier accordingly.\n",
        "    Report the mean f1 score of all the iterations.\n",
        "    \n",
        "    Note that you need to set random_state for ShuffleSplit\n",
        "    \"\"\"\n",
        "\n",
        "    # your code here\n",
        "    raise NotImplementedError\n",
        "\n",
        "    \n",
        "def main():\n",
        "    X,Y = utils.get_data_from_svmlight(\"features_svmlight.train\")\n",
        "    print(\"Classifier: SVD\")\n",
        "    f1_k = get_f1_kfold(X,Y)\n",
        "    print((\"Average F1 Score in KFold CV: \"+str(f1_k)))\n",
        "    f1_r = get_f1_randomisedCV(X,Y)\n",
        "    print((\"Average F1 Score in Randomised CV: \"+str(f1_r)))\n",
        "\n",
        "\n",
        "main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-17T04:30:21.105767Z",
          "start_time": "2022-01-17T04:30:20.381024Z"
        },
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "35f7fbef1e0f7512541358d3a261b838",
          "grade": true,
          "grade_id": "cell-c7bfbc42bf80f768",
          "locked": true,
          "points": 20,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "'''\n",
        "AUTOGRADER CELL. DO NOT MODIFY THIS.\n",
        "'''\n",
        "\n",
        "from numpy.testing import assert_almost_equal\n",
        "\n",
        "### 3.2 Cross Validation F1 [10 points]\n",
        "# test_f1_cv_kfold\n",
        "expected = 0.7258461959533061\n",
        "X, Y = get_data_from_svmlight(\"features_svmlight.train\")\n",
        "actual = get_f1_kfold(X, Y)\n",
        "assert_almost_equal(actual, expected, decimal=2, verbose=False, err_msg=\"test_f1_cv_kfold failed!\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-17T04:30:21.109723Z",
          "start_time": "2022-01-17T04:30:21.106920Z"
        },
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "cb382fd95d36e5a8b937b7769bb7c5c0",
          "grade": true,
          "grade_id": "cell-8ebda70b8d984a2f",
          "locked": true,
          "points": 0,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "dl4h_mooc_2111",
      "language": "python",
      "name": "dl4h_mooc_2111"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": false,
      "sideBar": true,
      "skip_h1_title": true,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {
        "height": "832px",
        "left": "419px",
        "top": "110px",
        "width": "311.390625px"
      },
      "toc_section_display": true,
      "toc_window_display": true
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}